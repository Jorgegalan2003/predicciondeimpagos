Códigos empleados en el Trabajo Fin de Grado de Jorge Galán para la ETSIT UPM, cuyo objetivo es el de la predicción de impagos en proyectos de electrificación rural de la Fundación Acciona.

En primer lugar, está clientes_y_ventas.py. Este código carga tres archivos CSV que contienen información sobre ventas, servicios y clientes. Tras verificar que los archivos existen, extrae las columnas clave, las renombra y realiza uniones entre las tablas para relacionar cada venta con su servicio correspondiente y, a su vez, con el cliente asociado. Finalmente, reorganiza las columnas resultantes (client_id, utility_id, sale_id) y guarda esta información consolidada en un nuevo archivo CSV llamado client_util_sale.csv
Después, se creo el módulo clientes_y_ventas_ordenado.py, que lo que hace es ordenar el CSV generado con el módulo anterior y ordenarlo  por cliente para que todas las ventas de cada cliente aparezcan de forma consecutiva.

Los módulos rates, quantity, nonPaymentPeriod, con_comunidad_y_país y amount añaden al CSV anterior las propiedades de ese mismo nombre al CSV. Toda esat funcionalidad se recoge en csv_completo.py, que hace simultáneamente todas las funcionalidades explicadas anteriormente.

Después, está el modulo filtro.py, que elimina todas aquellas ventas que tengan alguna de las variables independientes anteriormente mencionadas con valor NULL. El módulo definitivo.py realiza todas las funcionalidades explicadas hasta el momento, incluida este ultimo filtrado y además ordena las ventas de cada cliente por orden cronológico, de la más antigua a la más reciente.
Por su parte, el módulo DATOS_REALES.py extrae de la base de datos los estados reales de las ventas pertenecientes al conjunto de ventas de validación, ya sea a nivel global o en el estudio por cliente, y añadirlos al CSV de datos, para luego poder entrenar y evaluar el modelo, sus métricas y precisión.

Los siguientes scripts forman parte de un sistema completo para entrenar, evaluar y analizar modelos LSTM orientados a predecir el estado de pago de clientes (como "CORRECTO", "PENDIENTE_DE_PAGO" e "IMPAGO") a partir de datos de ventas y consumo. El flujo comienza con el entrenamiento y predicción mediante modelos LSTM como los de LSTM.py, LSTM_POR_PESOS.py y lstm_distribución_manual_de_pesos.py, que siguen una estructura similar: cargan datos de clientes, normalizan los valores de entrada, los dividen en entrenamiento y validación, y entrenan un modelo LSTM con PyTorch. Las diferencias radican en el tratamiento de las clases desbalanceadas: uno usa pesos por defecto, otro calcula pesos inversamente proporcionales a la frecuencia de las clases, y otro los asigna manualmente para enfatizar la detección de impagos. Se hizo una primera aproximación usando todos los clientes para tener una primera idea de qué método podría llegar a funcionar mejor, y se apreció que pesos inversos y pesos manuales tenían resultados similares.

Posteriormente, una vez se había comprobado que para esta caso de estudio el mejor de los escenarios planteados en el párrafo anterior era usar una distribución de pesos inversis a la frecuencia, se plantearon 4 escenarios en los que se trabajaría con los datos de Perú al ser los más desbalanceados: priorizar la Validation Loss usasndo pesos inversos y manuales, y priorizar el Recall de la clase Impago volviendo a usar las dos distintas distribuciones de pesos. Esta funcionalidad la dan los módulos predict_autoregresive.py, predict_autoregresive_pesos_manuales, prioritize_recall_pesos_inversos y prioritize_recall_pesos_manueales respectivamente. Los 4 módulos usan LSTM para entrenar el modelo y predecir el estado de las ventas, devolviendo el reporte de clasifificación de cada uno de los escenarios.
